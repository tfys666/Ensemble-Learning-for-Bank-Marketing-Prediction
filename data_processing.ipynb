{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ec2265a-dffe-45a2-8dc2-f1756659e6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "088aa4a1-68a5-46f5-9146-f8dc0c4b11e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data_path = \"original_data/bank-additional-full.csv\"\n",
    "# 小样本数据集：专门用于测试计算密集型算法（如SVM）\n",
    "# original_data_path = \"original_data/bank-additional.csv\"\n",
    "processed_data_path = \"processed_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba9dea84-caac-4be9-b804-12eb8bdecc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               age      duration      campaign         pdays      previous  \\\n",
      "count  41188.00000  41188.000000  41188.000000  41188.000000  41188.000000   \n",
      "mean      40.02406    258.285010      2.567593     -0.741988      0.172963   \n",
      "std       10.42125    259.279249      2.770014      1.510327      0.494901   \n",
      "min       17.00000      0.000000      1.000000     -1.000000      0.000000   \n",
      "25%       32.00000    102.000000      1.000000     -1.000000      0.000000   \n",
      "50%       38.00000    180.000000      2.000000     -1.000000      0.000000   \n",
      "75%       47.00000    319.000000      3.000000     -1.000000      0.000000   \n",
      "max       98.00000   4918.000000     56.000000     27.000000      7.000000   \n",
      "\n",
      "       emp.var.rate  cons.price.idx  cons.conf.idx     euribor3m   nr.employed  \n",
      "count  41188.000000    41188.000000   41188.000000  41188.000000  41188.000000  \n",
      "mean       0.081886       93.575664     -40.502600      3.621291   5167.035911  \n",
      "std        1.570960        0.578840       4.628198      1.734447     72.251528  \n",
      "min       -3.400000       92.201000     -50.800000      0.634000   4963.600000  \n",
      "25%       -1.800000       93.075000     -42.700000      1.344000   5099.100000  \n",
      "50%        1.100000       93.749000     -41.800000      4.857000   5191.000000  \n",
      "75%        1.400000       93.994000     -36.400000      4.961000   5228.100000  \n",
      "max        1.400000       94.767000     -26.900000      5.045000   5228.100000  \n"
     ]
    }
   ],
   "source": [
    "bank_data_small  = pd.read_csv(original_data_path,sep=\";\") \n",
    "bank_data_small['pdays'] = bank_data_small['pdays'].replace(999, -1)\n",
    "print(bank_data_small.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "337aad54-f0fe-4100-8175-3b6cabd44307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照数据类型对特征进行分类（连续/离散）\n",
    "string_features = bank_data_small.columns[bank_data_small.dtypes  == \"object\"].to_series().values\n",
    "int_features = bank_data_small.columns[bank_data_small.dtypes  == \"int64\"].to_series().values\n",
    "float_features = bank_data_small.columns[bank_data_small.dtypes  == \"float64\"].to_series().values\n",
    "numeric_features = np.append(int_features,float_features)\n",
    "\n",
    "bin_features = ['default', 'housing', 'loan','y']\n",
    "order_features = ['education']\n",
    "disorder_features = ['poutcome', 'job', 'marital', 'contact', 'month','day_of_week']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f4a12f-ca0f-4687-89c7-8da236df7d69",
   "metadata": {},
   "source": [
    "三种不同的缺失值填补方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf0ae68b-bb0e-48cd-9c85-61f05b405eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用均值填补缺失值\n",
    "def Missing_value_perprocessing_mean (bank_data_small_train,bank_data_small_test):\n",
    "    col  = bank_data_small_train.columns\n",
    "    #Train_copy = Train.copy()\n",
    "    #直接使用平均值填补缺失值\n",
    "    from sklearn.preprocessing import Imputer\n",
    "    imp = Imputer(missing_values=np.nan, strategy='mean', axis=0)\n",
    "    imp.fit(bank_data_small_train)\n",
    "    bank_data_small_train = imp.transform(bank_data_small_train) \n",
    "    bank_data_small_test = imp.transform(bank_data_small_test) \n",
    "    bank_data_small_train = pd.DataFrame(bank_data_small_train,columns = col)\n",
    "    bank_data_small_test = pd.DataFrame(bank_data_small_test,columns = col)\n",
    "    return bank_data_small_train,bank_data_small_test \n",
    "\n",
    "#使用随机森林填补缺失值\n",
    "def Missing_value_perprocessing_rf (bank_data_small_train,bank_data_small_test):\n",
    "    Missing_features_dict = {}\n",
    "    Missing_features_name = []\n",
    "    #先统计哪些列存在缺失的数据\n",
    "    for feature in bank_data_small_train.columns:\n",
    "        Missing_count = bank_data_small_train[bank_data_small_train[feature].isnull()]['age'].count() \n",
    "        if Missing_count > 0:\n",
    "            # 统计包含缺失值的列\n",
    "            Missing_features_dict.update({feature: Missing_count})\n",
    "    #对缺失的数据列按照缺失值数量从少到多排序，先拟合缺失值少的列        \n",
    "    Missing_features_name = sorted(Missing_features_dict.keys(),reverse=True) \n",
    "    #print(Missing_features_name)\n",
    "    for feature in Missing_features_name:     \n",
    "        #训练集中有缺失值的数据\n",
    "        train_miss_data = bank_data_small_train[bank_data_small_train[feature].isnull()]\n",
    "        train_miss_data_X = train_miss_data.drop(Missing_features_name, axis=1)\n",
    "        #训练集中没有缺失值的数据\n",
    "        train_full_data = bank_data_small_train[bank_data_small_train[feature].notnull()]     \n",
    "        train_full_data_Y = train_full_data[feature]\n",
    "        train_full_data_X = train_full_data.drop(Missing_features_name, axis=1)\n",
    "        #测试集中有缺失值的数据\n",
    "        test_miss_data = bank_data_small_test[bank_data_small_test[feature].isnull()]\n",
    "        test_miss_data_X = test_miss_data.drop(Missing_features_name, axis=1)\n",
    "        #测试集中没有缺失值的数据\n",
    "        test_full_data = bank_data_small_test[bank_data_small_test[feature].notnull()]     \n",
    "        test_full_data_Y = test_full_data[feature]\n",
    "        test_full_data_X = test_full_data.drop(Missing_features_name, axis=1)\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        #使用随机森林拟合        \n",
    "        rf = RandomForestClassifier(n_estimators=100)\n",
    "        #利用训练集中没有缺失值的数据构建随机森林\n",
    "        rf.fit(train_full_data_X, train_full_data_Y)\n",
    "        #预测训练集中的缺失值\n",
    "        train_miss_data_Y = rf.predict(train_miss_data_X)\n",
    "        #预测测试集中的缺失值\n",
    "        test_miss_data_Y = rf.predict(test_miss_data_X) \n",
    "        #将训练集中的缺失值补充完整\n",
    "        train_miss_data[feature] = train_miss_data_Y  \n",
    "        #将测试集中的缺失值补充完整\n",
    "        test_miss_data[feature] = test_miss_data_Y \n",
    "        #将补充完整的\n",
    "        bank_data_small_train = pd.concat([train_full_data, train_miss_data])\n",
    "        bank_data_small_test = pd.concat([test_full_data, test_miss_data])      \n",
    "        \n",
    "    return bank_data_small_train,bank_data_small_test\n",
    "\n",
    "#使用knn填补缺失值\n",
    "def Missing_value_perprocessing_knn (bank_data_small_train,bank_data_small_test):\n",
    "    Missing_features_dict = {}\n",
    "    Missing_features_name = []\n",
    "    #先统计哪些列存在缺失的数据\n",
    "    for feature in bank_data_small_train.columns:\n",
    "        Missing_count = bank_data_small_train[bank_data_small_train[feature].isnull()]['age'].count() \n",
    "        if Missing_count > 0:\n",
    "            # 统计包含缺失值的列\n",
    "            Missing_features_dict.update({feature: Missing_count})\n",
    "    #对缺失的数据列按照缺失值数量从少到多排序，先拟合缺失值少的列        \n",
    "    Missing_features_name = sorted(Missing_features_dict.keys(),reverse=True)\n",
    "    from sklearn.neighbors import KNeighborsClassifier \n",
    "    for feature in Missing_features_name:     \n",
    "        #训练集中有缺失值的数据\n",
    "        train_miss_data = bank_data_small_train[bank_data_small_train[feature].isnull()]\n",
    "        train_miss_data_X = train_miss_data.drop(Missing_features_name, axis=1)\n",
    "        #训练集中没有缺失值的数据\n",
    "        train_full_data = bank_data_small_train[bank_data_small_train[feature].notnull()]     \n",
    "        train_full_data_Y = train_full_data[feature]\n",
    "        train_full_data_X = train_full_data.drop(Missing_features_name, axis=1)\n",
    "        #测试集中有缺失值的数据\n",
    "        test_miss_data = bank_data_small_test[bank_data_small_test[feature].isnull()]\n",
    "        test_miss_data_X = test_miss_data.drop(Missing_features_name, axis=1)\n",
    "        #测试集中没有缺失值的数据\n",
    "        test_full_data = bank_data_small_test[bank_data_small_test[feature].notnull()]     \n",
    "        test_full_data_Y = test_full_data[feature]\n",
    "        test_full_data_X = test_full_data.drop(Missing_features_name, axis=1)\n",
    "        \n",
    "        #使用K近邻拟合        \n",
    "        knn = KNeighborsClassifier()\n",
    "        forest = knn.fit(train_full_data_X, train_full_data_Y)\n",
    "        \n",
    "        train_miss_data_Y = knn.predict(train_miss_data_X)\n",
    "        test_miss_data_Y = knn.predict(test_miss_data_X) \n",
    "        \n",
    "        train_miss_data.loc[:, feature] = train_miss_data_Y\n",
    "        test_miss_data.loc[:, feature] = test_miss_data_Y\n",
    "\n",
    "        bank_data_small_train = pd.concat([train_full_data, train_miss_data])\n",
    "        bank_data_small_test = pd.concat([test_full_data, test_miss_data])      \n",
    "        \n",
    "    return bank_data_small_train,bank_data_small_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7f2deac-0594-4a49-b07f-8b58f01bd556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把二分类特征转化成（1，0）\n",
    "def bin_features_perprocessing (bin_features, bank_data):\n",
    "    for feature in bin_features:      \n",
    "        new = np.zeros(bank_data[feature].shape[0])\n",
    "        for rol in range(bank_data[feature].shape[0]):\n",
    "            if bank_data[feature][rol] == 'yes' :\n",
    "                new[rol] = 1\n",
    "            elif bank_data[feature][rol]  == 'no':\n",
    "                new[rol] = 0\n",
    "            else:\n",
    "                new[rol] = None\n",
    "        bank_data[feature] =  new   \n",
    "    return bank_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fffc4dde-7903-4a78-863c-efb325bfd1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#特征值有次序关系的特征，按照特征值强弱排序（1，2，……，n）（如：受教育程度）\n",
    "def order_features_perprocessing (order_features,bank_data):\n",
    "    education_values = [\"illiterate\", \"basic.4y\", \"basic.6y\", \"basic.9y\", \n",
    "    \"high.school\",  \"professional.course\", \"university.degree\",\"unknown\"]\n",
    "    replace_values = list(range(1,  len(education_values)))\n",
    "    replace_values.append(None)\n",
    "    bank_data[order_features] = bank_data[order_features].replace(education_values,replace_values)\n",
    "    bank_data[order_features] = bank_data[order_features].astype(\"float\")\n",
    "    return bank_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8dedc2a3-b548-4ae7-9e5e-8775af02ce1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#特征值没有次序的特征，使用onehot编码\n",
    "def disorder_features_perprocessing (disorder_features, bank_data):\n",
    "    for features in disorder_features:\n",
    "        #做onehot\n",
    "        features_onehot = pd.get_dummies(bank_data[features])\n",
    "        #把名字改成features_values\n",
    "        features_onehot = features_onehot.rename(columns=lambda x: features+'_'+str(x))\n",
    "        #拼接onehot得到的新features\n",
    "        bank_data = pd.concat([bank_data,features_onehot],axis=1)\n",
    "        #删掉原来的feature columns\n",
    "        bank_data = bank_data.drop(features, axis=1)\n",
    "    return bank_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c8902fa-8ff8-439d-8cfd-44a556325dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#归一化\n",
    "def Scale_perprocessing (Train):\n",
    "    col  = Train.columns\n",
    "    copy = Train.copy()\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    copy = scaler.fit_transform(copy)\n",
    "    Train = pd.DataFrame(copy,columns = col)\n",
    "    return Train "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3797b9c1-aa2b-45aa-a179-bb62b1316808",
   "metadata": {},
   "source": [
    "使用上面定义的一系列函数来处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "299c1e6f-4092-436b-902b-cba42d5f3b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#将字符型特征转化为数值型特征\n",
    "#转化二分类特征为1，0\n",
    "bank_data_small = bin_features_perprocessing(bin_features, bank_data_small)\n",
    "#转化包含次序的特征\n",
    "bank_data_small = order_features_perprocessing(order_features, bank_data_small)\n",
    "#转化无序的特征\n",
    "bank_data_small = disorder_features_perprocessing(disorder_features, bank_data_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f05c0b9d-ef4d-4ce4-bdf7-a93c8ce9ed34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#随机重排后，划分训练集和测试集\n",
    "bank_data_small.shape[0]\n",
    "round(bank_data_small.shape[0]*0.8)\n",
    "bank_data_small = bank_data_small.sample(frac=1,random_state=12)\n",
    "import math\n",
    "bank_data_small_train = bank_data_small.iloc[0:round(bank_data_small.shape[0]*0.8),:]\n",
    "bank_data_small_test = bank_data_small.iloc[round(bank_data_small.shape[0]*0.8):,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "460cdca1-6132-438c-b950-c7b951e1aa73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#缺失数据处理\n",
    "#平均值\n",
    "#bank_data_small_train,bank_data_small_test = Missing_value_perprocessing_mean(bank_data_small_train,bank_data_small_test)\n",
    "#k近邻\n",
    "bank_data_small_train,bank_data_small_test = Missing_value_perprocessing_knn(bank_data_small_train,bank_data_small_test)\n",
    "#随机森林\n",
    "#bank_data_small_train,bank_data_small_test = Missing_value_perprocessing_rf(bank_data_small_train,bank_data_small_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8bc9c130-87b4-4a63-a809-b5288746db8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_small = bank_data_small_train.drop(['y'], axis=1).copy()\n",
    "y_train_small = pd.DataFrame(bank_data_small_train['y'],columns = ['y'])\n",
    "\n",
    "X_test_small = bank_data_small_test.drop(['y'], axis=1).copy()\n",
    "y_test_small = pd.DataFrame(bank_data_small_test['y'],columns = ['y'])\n",
    "\n",
    "X_train_small = Scale_perprocessing(X_train_small)\n",
    "X_test_small = Scale_perprocessing(X_test_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0bd0c71-635c-4b59-86cc-dee65307eb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#导出\n",
    "X_test_output_path = \"processed_data/X_test.csv\"\n",
    "y_test_output_path = \"processed_data/y_test.csv\"\n",
    "X_train_output_path = \"processed_data/X_train.csv\"\n",
    "y_train_output_path = \"processed_data/y_train.csv\"\n",
    "\n",
    "# X_test_output_path = \"processed_data/X_test_small.csv\"\n",
    "# y_test_output_path = \"processed_data/y_test_small.csv\"\n",
    "# X_train_output_path = \"processed_data/X_train_small.csv\"\n",
    "# y_train_output_path = \"processed_data/y_train_small.csv\"\n",
    "\n",
    "X_test_small.to_csv(X_test_output_path,index = False)\n",
    "y_test_small.to_csv(y_test_output_path,index = False)\n",
    "X_train_small.to_csv(X_train_output_path,index = False)\n",
    "y_train_small.to_csv(y_train_output_path,index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d151cd47-d11a-4568-8782-bb6a2d2b01bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
